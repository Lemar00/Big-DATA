{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming structuré à l'aide de l'API Python DataFrames\n",
        "\n",
        "Apache Spark inclut une API de haut niveau dédiée au traitement des stream, [Structured Streaming](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html). Dans ce notebook, nous examinons rapidement comment utiliser l'API DataFrame pour créer des applications de streaming structuré. Nous voulons calculer des métriques en temps réel comme les décomptes en cours d'exécution et les décomptes fenêtrés sur un flux d'actions horodatées (par exemple, ouvrir, fermer, etc.).\n",
        "\n",
        "Pour exécuter ce notebook, importez-le et attachez-le à un cluster Spark."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7a5776cc-1da2-46c2-82fc-a3db8e5a04c4"
        },
        "id": "xNmvGspz5ynF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uF7rQtTuhRhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configurer PySpark dans Colab\n",
        "Installer Pyspark et findspark.\n",
        "findspark est important ici; Il localisera Spark sur le système et l'importera en tant que bibliothèque standard."
      ],
      "metadata": {
        "id": "ENkH5AmPihEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark\n",
        "print(\"installé\")"
      ],
      "metadata": {
        "id": "bYO20vykif0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.9/dist-packages/pyspark\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
      ],
      "metadata": {
        "id": "4rUMx-BIiwUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data\n",
        "Nous avons quelques exemples de données d'action sous forme de fichiers dans `events/` que nous allons utiliser pour créer cette application. Examinons le contenu de ce répertoire."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "39eb54e7-59e7-4217-9619-79e4bf885027"
        },
        "id": "qmlKjJLl5ynJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir = '/content/drive/MyDrive/tpspark/events'  #Attention !!! Mettez le chemin qui vous correspond.\n",
        "os.listdir(dataDir)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5a9ba90c-2b0d-4385-ab09-1b21179c000d"
        },
        "id": "sslRDt0n5ynJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2n2mPI0k7m-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il y a environ 50 fichiers JSON dans le répertoire. Voyons ce que contient chaque fichier JSON."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f4b333bd-50e7-43d5-af62-5a5fa699770a"
        },
        "id": "wnFKWzXp5ynK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head '/content/drive/MyDrive/tpspark/events/file-0.json'"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d0d0af05-2226-48ba-b6fd-713146ac56fb"
        },
        "id": "AvdFX9PB5ynL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chaque ligne du fichier contient un enregistrement JSON avec deux champs - 'time' et 'action'. Essayons d'analyser ces fichiers de manière interactive."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "473a6faa-75e9-43d1-8cdc-e16431e44a13"
        },
        "id": "JuCEijwp5ynL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialisation / Configuration / démarrage de Spark\n"
      ],
      "metadata": {
        "id": "ZTDWMklUlPrg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFu44fW7LRgC"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "# Definition des focntions display pour un meilleur affichage\n",
        "\n",
        "def display(df, n=100):\n",
        "  return data_table.DataTable(df.limit(n).toPandas(), include_index=False, num_rows_per_page=10)\n",
        "\n",
        "def display2(df, n=20):\n",
        "  pd.set_option('max_columns', None)\n",
        "  pd.set_option('max_colwidth', None)\n",
        "  return df.limit(n).toPandas().head(n)\n",
        "\n",
        "print(\"display redéfini\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de Spark\n",
        "import findspark\n",
        "print(\"findspark.init() initialise les variables d'environnement pour spark\")\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "\n",
        "# pour les dataframe et udf\n",
        "from pyspark.sql import *\n",
        "from datetime import *"
      ],
      "metadata": {
        "id": "JgxdRrypb1tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Démarrage session spark\n",
        "# --------------------------\n",
        "def demarrer_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP\"\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local)\n",
        "\n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "\n",
        "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (4 coeurs)\n",
        "  print(\"session démarrée, son id est \", sc.applicationId)\n",
        "  return spark\n",
        "\n",
        "spark = demarrer_spark()"
      ],
      "metadata": {
        "id": "3L5eQPqtcljK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch/Interactive Processing\n",
        "La première étape habituelle pour tenter de traiter les données consiste à interroger les données de manière interactive. Définissons un DataFrame statique sur les fichiers et donnons-lui un nom de table."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "804bc3ee-1437-461b-8395-e9d18b32a8f3"
        },
        "id": "CX2QqZ6q5ynL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.sql.types import *\n",
        "\n",
        "inputPath = \"/content/drive/MyDrive/tpspark/events/\"\n",
        "\n",
        "# Puisque nous connaissons déjà le format des données, définissons le schéma pour accélérer le traitement (pas besoin de Spark pour déduire le schéma)\n",
        "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
        "\n",
        "# DataFrame statique représentant les données dans les fichiers JSON\n",
        "staticInputDF = (\n",
        "  spark\n",
        "    .read\n",
        "    .schema(jsonSchema)\n",
        "    .json(inputPath)\n",
        ")\n",
        "\n",
        "display(staticInputDF)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "052de597-ddb8-4ff4-9693-57b6706c5156"
        },
        "id": "MGK2So2s5ynM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons maintenant calculer le nombre d'actions \"open\" et \"close\" avec des fenêtres d'une heure. Pour ce faire, nous allons regrouper par la colonne d'action et une fenêtres d'une heure sur la colonne \"time\"."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0d5b179e-2575-4d6e-a05f-30949e5c9806"
        },
        "id": "IG909eiS5ynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *      # pour la fonction window()\n",
        "\n",
        "staticCountsDF = (\n",
        "  staticInputDF\n",
        "    .groupBy(\n",
        "       staticInputDF.action,\n",
        "       window(staticInputDF.time, \"1 hour\"))\n",
        "    .count()\n",
        ")\n",
        "staticCountsDF.cache()\n",
        "\n",
        "# A partir du DF, créer une table vistuelle 'static_counts'\n",
        "staticCountsDF.createOrReplaceTempView(\"static_counts\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d856da4a-d352-42c0-9701-fe91a7eef5e2"
        },
        "id": "tfRaADdr5ynN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons maintenant utiliser directement SQL pour interroger la table. Par exemple, voici les décomptes totaux pour toutes les heures."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b3627362-e104-40da-b0f9-b764cb0388d1"
        },
        "id": "GSJsUPrR5ynN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql select action, sum(count) as total_count from static_counts group by action\n",
        "spark.sql(\"\"\" select action, sum(count) as total_count from static_counts group by action \"\"\")show()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a9846312-306a-45d8-84ac-de7657ae4e77"
        },
        "id": "JSZWbWFT5ynO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que dire d'une chronologie des décomptes fenêtrés ?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "19b663d7-aa35-4086-87a9-7fa963df2541"
        },
        "id": "ha39L6_j5ynO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "31ba8161-4316-4084-96e8-a0857c2ec6fe"
        },
        "id": "4YICGrp-5ynP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stream Processing\n",
        "Maintenant que nous avons analysé les données de manière interactive, convertissons cela en une requête en continu qui se met à jour en continu au fur et à mesure que les données arrivent. Puisque nous n'avons qu'un ensemble statique de fichiers, nous allons émuler un flux à partir d'eux en lisant un fichier à la fois, dans l'ordre chronologique de leur création. La requête que nous devons écrire est à peu près la même que la requête interactive ci-dessus."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fda2a865-86b6-4963-af5f-385bd2334e25"
        },
        "id": "s0llEwVT5ynP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Semblable à la définition de staticInputDF ci-dessus, en utilisant simplement `readStream` au lieu de `read\n",
        "streamingInputDF = (\n",
        "  spark\n",
        "    .readStream\n",
        "    .schema(jsonSchema)               # Définir le schéma des données JSON\n",
        "    .option(\"maxFilesPerTrigger\", 1)  # Traiter une séquence de fichiers comme un flux en sélectionnant un fichier à la fois\n",
        "    .json(inputPath)\n",
        ")\n",
        "\n",
        "# Même requête que staticInputDF\n",
        "streamingCountsDF= (\n",
        "  streamingInputDF\n",
        "    .groupBy(\n",
        "      streamingInputDF.action,\n",
        "      window(streamingInputDF.time, \"1 hour\"))\n",
        "    .count()\n",
        ")\n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1a940eea-2d4d-431c-ad2e-d9cedcb068d3"
        },
        "id": "IEuJTTSj5ynP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ce DF est-il réellement un DF en streaming ?\n",
        "streamingCountsDF1.isStreaming"
      ],
      "metadata": {
        "id": "1DeeR-lCo2aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme vous pouvez le voir, streamingCountsDF est une dataframe streaming (streamingCountsDF.isStreaming 'is true'). Vous pouvez démarrer l'exécution en streaming en définissant le 'sink' (récepteur) et en le démarrant. Dans notre cas, nous voulons interroger les décomptes de manière interactive (mêmes requêtes que ci-dessus), nous allons donc définir l'ensemble complet des décomptes d'une heure dans une table en mémoire."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1ccbd0af-9c8c-4df2-9387-f1afc5edbcdd"
        },
        "id": "Ffib_eYf5ynQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # garder la taille des \"shuffle\" petite\n",
        "query = (\n",
        "  streamingCountsDF\n",
        "    .writeStream\n",
        "    .format(\"memory\")        # mémoire = stocker la table en mémoire\n",
        "    .queryName(\"counts\")     # counts = nom de la table en mémoire\n",
        "    .outputMode(\"complete\")  # complet = tous les comptages doivent être dans la table\n",
        "    .start()\n",
        "    )"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fe9e4efa-f07a-4474-a76c-70ef9ba85558"
        },
        "id": "iL51kRqL5ynQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "`query` est un handle de la requête de streaming qui s'exécute en arrière-plan. Cette requête récupère en permanence des fichiers et met à jour les décomptes fenêtrés.\n",
        "\n",
        "Notez l'état de 'uery' dans la cellule ci-dessus. La barre de progression indique que la requête est active.\n",
        "De plus, si vous développez le `> counts` ci-dessus, vous trouverez le nombre de fichiers qu'ils ont déjà traités.\n",
        "\n",
        "Attendons un peu que quelques fichiers soient traités, puis interrogeons de manière interactive la table `counts` en mémoire."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "36736a03-fed5-4173-9720-6d94659083ab"
        },
        "id": "OweLJqT45ynR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "sleep(5)  # attendre un peu que le calcul démarre"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fc5b6fc1-f142-4ff8-98de-e719c9405b40"
        },
        "id": "9dOJ0tQW5ynR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0z8Jmas1cPD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6fee0151-474e-44c5-90ae-24bd154dde16"
        },
        "id": "fe-YNcic5ynR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous voyons la chronologie des comptages fenêtrés (similaire au comptage statique précédent) s'accumuler. Si nous continuons à exécuter cette requête interactive à plusieurs reprises, nous verrons les derniers décomptes mis à jour que la requête de streaming met à jour en arrière-plan."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4642a107-c2ac-460f-9bea-1d0f6f4b240d"
        },
        "id": "1hDJ0G_35ynS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sleep(5)  # attendre un peu plus pour que plus de données soient calculées"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "49a730c8-66e0-41e3-a0e9-63dff4a52244"
        },
        "id": "FyrQietY5ynS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0O7l5jG_uc3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b72203c7-fffc-4595-8df5-adb442374e23"
        },
        "id": "Sk3xDoJ85ynS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sleep(5)  # attendre un peu plus pour que plus de données soient calculées"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f88e440f-6de5-421a-9b37-dbbf455324ac"
        },
        "id": "ZRK81pUo5ynS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4a4b8d5-4779-431a-a12c-4f59d683f3f2"
        },
        "id": "NLIyiaHb5ynT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "::Voyons également le nombre total d'\"open\" et de \"close\"."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e5caecec-2eb9-4d1f-8694-89760b28abbf"
        },
        "id": "5pYldclp5ynT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select action, sum(count) as total_count from counts group by action order by action"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4eb9131c-58b5-4d84-90f7-0dd5fc06f079"
        },
        "id": "I28FrbRe5ynT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vous continuez à exécuter la requête ci-dessus à plusieurs reprises, vous constaterez toujours que le nombre d'\"opens\" est supérieur au nombre de \"closes\", comme prévu dans un flux de données où une \"closes\" apparaît toujours après l'\"open\" correspondante. Cela montre que le streaming structuré garantit le **prefix integrity**. Lisez les articles de blog liés ci-dessous si vous voulez en savoir plus.\n",
        "\n",
        "Notez qu'il n'y a que quelques fichiers, donc en les consommant tous, il n'y aura pas de mises à jour des décomptes. Réexécutez la requête si vous souhaitez interagir à nouveau avec la requête de diffusion en continu.\n",
        "\n",
        "Enfin, vous pouvez arrêter l'exécution de la requête en arrière-plan, soit en cliquant sur le lien 'Annuler' dans la cellule de la requête, soit en exécutant `query.stop()`. Dans les deux cas, lorsque la requête est arrêtée, le statut de la cellule correspondante ci-dessus sera automatiquement mis à jour vers `TERMINATED`."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68cde8b1-799f-4244-bd32-148ec58260c9"
        },
        "id": "W1yLs1875ynU"
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Structured Streaming using Python DataFrames",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 1268709608023288
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}